{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test 데이터셋을 불러오는 클래스\n",
    "def split_dataset(label_file, test_size=0.2, shuffle=True):\n",
    "\n",
    "    # CSV 파일에서 이미지와 레이블 데이터를 불러옴\n",
    "    labels = pd.read_csv(label_file)\n",
    "\n",
    "    # train, test 데이터를 나눔\n",
    "    train_labels, test_labels = train_test_split(labels, test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    return train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 자/모음 vocabulary와 이를 index로 반환하는 함수\n",
    "# jamo_vocabulary = ['ㅎ', 'ㅔ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', \n",
    "#              'ㅕ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', \n",
    "#              'ㅖ', 'ㄱ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ', 'ㅏ', 'ㅐ', \n",
    "#              'ㅃ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅝ', 'ㅞ', 'ㄸ', 'ㅜ', 'ㅉ', '<sos>', '<eos>', '<pad>']\n",
    "\n",
    "# jamoindex = {word: i for i, word in enumerate(jamo_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자/모음을 index로 변환하는 클래스\n",
    "class jamo_to_index:\n",
    "  def __init__(self):\n",
    "    self.jamo_vocabulary = ['ㅎ', 'ㅔ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', \n",
    "             'ㅕ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', \n",
    "             'ㅖ', 'ㄱ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ', 'ㅏ', 'ㅐ', \n",
    "             'ㅃ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅝ', 'ㅞ', 'ㄸ', 'ㅜ', 'ㅉ', '<sos>', '<eos>', '<pad>']\n",
    "    self.jamo_to_index = {word: i for i, word in enumerate(self.jamo_vocabulary)}\n",
    "\n",
    "  def process(self, label):\n",
    "    label_indices = [self.jamo_to_index[char] for char in label]\n",
    "\n",
    "    src_target = [self.jamo_to_index['<sos>']] + label_indices\n",
    "    src_target_tensor = torch.tensor(src_target, dtype=torch.long)\n",
    "\n",
    "    tgt_target = label_indices + [self.jamo_to_index['<eos>']]\n",
    "    tgt_target_tensor = torch.tensor(tgt_target, dtype=torch.long)\n",
    "\n",
    "    return src_target_tensor, tgt_target_tensor\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(jamo_to_index().process('ㅎㅔㄹ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequence를 추가하는 함수\n",
    "def collate_fn(batch, pad_idx):\n",
    "  images, src_labels, tgt_labels = zip(*batch)\n",
    "\n",
    "  images = torch.stack(images)\n",
    "\n",
    "  src_labels_padded = pad_sequence(src_labels, batch_first=True, padding_value=pad_idx)\n",
    "  tgt_labels_padded = pad_sequence(tgt_labels, batch_first=True, padding_value=pad_idx)\n",
    "\n",
    "  return images, src_labels_padded, tgt_labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangulOCRDataset(Dataset):\n",
    "  def __init__(self, img_dir, labels, transform = None, max_seq_len = 16, vocabulary = None):\n",
    "    self.img_dir = img_dir\n",
    "    self.labels = labels\n",
    "    self.transform = transform\n",
    "    self.vocabulary = jamo_to_index()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    # 데이터셋에서 인덱스에 해당하는 이미지와 레이블을 불러옴\n",
    "    img_name = os.path.join(self.img_dir, self.labels.iloc[idx, 0])\n",
    "    image = Image.open(img_name)\n",
    "    label = self.labels.iloc[idx, 1]\n",
    "\n",
    "    src_label, tgt_label = self.vocabulary.process(label)\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    \n",
    "    return image, src_label, tgt_label\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_pad_mask(sequence, pad_idx = 53):\n",
    "#   # embedding하기 전 token index가 pad_idx인 경우 mask를 생성하는 함수\n",
    "\n",
    "#   mask = sequence.ne(pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "#   mask = mask.repeat(1, 1, sequence.size(1), 1)\n",
    "\n",
    "#   mask.requires_grad = False\n",
    "#   return mask # (batch_size, 1, tgt_len, tgt_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = torch.randint(52, 54, (2, 16, ))\n",
    "# print(sequence)\n",
    "# print(make_pad_mask(sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_subsequent_mask(seq_len, pad_idx = 53):\n",
    "#   # query는 target sequence, key는 source sequence\n",
    "\n",
    "#   tril = np.tril(np.ones((seq_len, seq_len)), k=0).astype('uint8') # 0인 부분이 mask가 되어야 함\n",
    "#   mask = torch.tensor(tril, dtype=torch.bool, requires_grad=False, device='cuda')\n",
    "#   return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(make_subsequent_mask(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_tgt_mask(self, tgt):\n",
    "#   pad_mask = self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JamoEmbedding(nn.Module):\n",
    "  def __init__(self, vocab_size = 54, embedding_dim = 512, pad_idx = None):\n",
    "    super(JamoEmbedding, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "    self.embedding_dim = embedding_dim\n",
    "  def forward(self, x):\n",
    "    return self.embedding(x) * math.sqrt(self.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNNEncoder, self).__init__()\n",
    "    # input_size: [batch_size, 1, 360, 360]\n",
    "    # output_size: [batch_size, 121, 512]\n",
    "\n",
    "    self.conv1 = nn.Sequential(\n",
    "      nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2) # [batch_size, 64, 180, 180]\n",
    "    )\n",
    "\n",
    "    self.conv2 = nn.Sequential(\n",
    "      nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2) # [batch_size, 128, 90, 90]\n",
    "    )\n",
    "\n",
    "    self.conv3 = nn.Sequential(\n",
    "      nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(256),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2) # [batch_size, 256, 45, 45]\n",
    "    )\n",
    "\n",
    "    self.conv4 = nn.Sequential(\n",
    "      nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(512),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=4, stride=4) # [batch_size, 512, 11, 11]\n",
    "    )\n",
    "\n",
    "  def flatten_feature_map(self, x):\n",
    "    batch_size, channels, height, width = x.size()\n",
    "    flattened_feature_map = x.view(batch_size, channels, height * width) # [batch_size, 512, 121]\n",
    "    transposed_feature_map = flattened_feature_map.permute(0, 2, 1) # [batch_size, 121, 512]\n",
    "    # 여기 learnable한 linear layer 추가 할지 말지는 나중에 결정\n",
    "\n",
    "    return transposed_feature_map\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.conv4(x)\n",
    "\n",
    "    return self.flatten_feature_map(x) # [batch_size, 121, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalFeedForward(nn.Module):\n",
    "  def __init__(self, embedding_dim = 512, d_ff = 2048, dropout = 0.1):\n",
    "    super(PositionalFeedForward, self).__init__()\n",
    "    self.fc1 = nn.Linear(embedding_dim, d_ff) # (d_embed, d_ff)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(d_ff, embedding_dim) # (d_ff, d_embed)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x\n",
    "    out = self.fc1(out)\n",
    "    out = self.relu(out)\n",
    "    out = self.dropout(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Residual, self).__init__()\n",
    "\n",
    "  def forward(self, x, sub_layer):\n",
    "    out = x\n",
    "    out = sub_layer(out)\n",
    "    out = out + x\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, self_attention, cross_attention, position_ff):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    self.self_attention = self_attention\n",
    "    self.cross_attention = cross_attention\n",
    "    self.position_ff = position_ff\n",
    "    self.residuals = [Residual() for _ in range(3)]\n",
    "\n",
    "  def forward(self, tgt, encoder_out, pad_mask, look_ahead_mask):\n",
    "     out = tgt # [batch_size, seq_len, d_embed = 512]\n",
    "     out = self.residuals[0](out, lambda out: self.self_attention(query = out, key = out, value = out, mask = (pad_mask & look_ahead_mask))) # tgt_mask는 make_subesequent_mask로 생성\n",
    "     out = self.residuals[1](out, lambda out: self.cross_attention(query = out, key = encoder_out, value = encoder_out, pad_mask = pad_mask)) # make_subesequent_mask로 생성\n",
    "     out = self.residuals[2](out, self.position_ff)\n",
    "\n",
    "     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, embedding_dim, num_heads, num_layers):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.decoder_layer = DecoderBlock(\n",
    "      \n",
    "      self_attention = nn.MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads),\n",
    "      cross_attention = nn.MultiheadAttention(embed_dim = embedding_dim, num_heads = num_heads),\n",
    "      position_ff = PositionalFeedForward(embedding_dim = embedding_dim)\n",
    "    )\n",
    "    self.layers = nn.ModuleList([copy.deepcopy(self.decoder_layer) for _ in range(num_layers)]) # DecoderBlock 객체를 num_layers만큼 생성\n",
    "    \n",
    "  def forward(self, src, encoder_out, pad_mask, look_ahead_mask):\n",
    "    out = src\n",
    "    for layer in self.layers:\n",
    "      out = layer(out, encoder_out, pad_mask, look_ahead_mask)\n",
    "\n",
    "    return out # decoder block을 num_layers만큼 통과한 결과\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "  def __init__(self, embedding_dim, num_heads, dropout = 0.1):\n",
    "    super(MultiHeadAttentionBlock, self).__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    assert embedding_dim % num_heads == 0 # embedding_dim이 num_heads로 나누어 떨어져야 함, 우리는 일단 head 수를 1로 할 것임.\n",
    "\n",
    "    self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "    self.q_fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "    self.k_fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "    self.v_fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  @staticmethod\n",
    "  def attention(self, query, key, value, mask, dropout : nn.Dropout):\n",
    "\n",
    "    d_k = key.size(-1)\n",
    "    \n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "      scores = scores.masked_fill(mask == 0, -1e9) # 0인 부분이 mask, -inf로 채움\n",
    "\n",
    "    scores = scores.softmax(dim = -1)\n",
    "\n",
    "    if dropout is not None:\n",
    "      scores = dropout(scores)\n",
    "\n",
    "    return torch.matmul(scores, value)\n",
    "  \n",
    "  def forward(self, query, key, value, mask):\n",
    "\n",
    "    # cross attention 할 때는 query는 decoder의 output, key와 value는 encoder의 output\n",
    "    # self attention 할 때는 query, key, value가 모두 같음\n",
    "    \n",
    "    batch_size = query.shape[0]\n",
    "\n",
    "    # query, key, value: [batch_size, seq_len, embedding_dim]\n",
    "    \n",
    "    def transform(x, fc):\n",
    "      out = fc(x)\n",
    "      out = out.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "      out = out.transpose(1, 2)\n",
    "      return out\n",
    "\n",
    "    query = transform(query, self.q_fc)\n",
    "    key = transform(key, self.k_fc)\n",
    "    value = transform(value, self.v_fc)\n",
    "\n",
    "\n",
    "    # Q: [batch_size, num_heads, seq_len, head_dim]\n",
    "    # K: [batch_size, num_heads, seq_len, head_dim]\n",
    "    # V: [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "    output = self.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "    # output: [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "    output = output.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    # output: [batch_size, seq_len, num_heads, head_dim]\n",
    "\n",
    "    output = output.view(batch_size, -1, self.embedding_dim)\n",
    "\n",
    "    # energy: [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "    return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 52, 52],\n",
      "        [53, 53, 53, 53, 52, 53, 52, 52, 53, 52, 53, 52, 53, 53, 53, 53]])\n",
      "tensor([[[[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "            True,  True,  True, False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False, False,  True, False,  True,  True, False,  True,\n",
      "           False,  True, False, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "# def make_pad_mask(sequence, pad_idx = 53):\n",
    "#         # sequence: [batch_size, seq_len]\n",
    "#         # embedding하기 전 token index가 pad_idx인 경우 mask를 생성하는 함수\n",
    "#         seq_len = sequence.size(1)\n",
    "\n",
    "#         mask = (sequence != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "#         mask.requires_grad = False\n",
    "\n",
    "#         return mask # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "# sequence = torch.randint(52, 54, (2, 16,))\n",
    "# print(sequence)\n",
    "# print(make_pad_mask(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangulViT(nn.Module):\n",
    "    def __init__(self, encoder, decoder, emb_size, nhead, vocab_size, num_decoder, dim_feedforward):\n",
    "        super(HangulViT, self).__init__()\n",
    "        self.encoder = encoder # Vision Transformer Encoder (나중에 구현 예정)\n",
    "        self.decoder = decoder # Transformer Decoder\n",
    "        self.num_decoder = num_decoder\n",
    "        self.emb_size = emb_size\n",
    "        self.nhead = 1\n",
    "        self.embedding = JamoEmbedding(vocab_size = vocab_size, embedding_dim = emb_size, pad_idx = 53) # 각 토큰 embedding\n",
    "        self.generator = nn.Linear(emb_size, vocab_size) # 자/모음 generation\n",
    "\n",
    "    def decode(self, src, encoder_out, pad_mask, look_ahead_mask):\n",
    "        return self.decoder(self.embedding(src), encoder_out, pad_mask, look_ahead_mask)\n",
    "\n",
    "    def make_pad_mask(sequence, pad_idx = 53):\n",
    "        # sequence: [batch_size, seq_len]\n",
    "        # embedding하기 전 token index가 pad_idx인 경우 mask를 생성하는 함수\n",
    "        seq_len = sequence.size(1)\n",
    "\n",
    "        mask = (sequence != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        mask.requires_grad = False\n",
    "\n",
    "        return mask # (batch_size, 1, 1, seq_len)\n",
    "    \n",
    "\n",
    "\n",
    "    def make_subsequent_mask(seq_len, pad_idx = 53):\n",
    "        # look_ahead masking\n",
    "\n",
    "        tril = np.tril(np.ones((seq_len, seq_len)), k=0).astype('uint8') # 0인 부분이 mask가 되어야 함\n",
    "        mask = torch.tensor(tril, dtype=torch.bool, requires_grad=False, device='cuda')\n",
    "        return mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        pad_mask = self.make_pad_mask(tgt, tgt)\n",
    "        seq_mask = self.make_subsequent_mask(tgt, tgt)\n",
    "        mask = pad_mask & seq_mask\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, image, src):\n",
    "\n",
    "        max_seq = src.size(1)\n",
    "        look_ahead_mask = self.make_subsequent_mask(max_seq) # Look-ahead mask\n",
    "        pad_mask = self.make_pad_mask(src) # attention 할때 사용할 mask\n",
    "\n",
    "        src = self.embedding(src)\n",
    "        # tgt = self.embedding(tgt)\n",
    "\n",
    "        feature_map = self.encoder(image) # [batch_size, 121, 512]\n",
    "\n",
    "        # q, k, v = feature_map, feature_map, feature_map\n",
    "\n",
    "        decoder_out = self.decode(src, encoder_out = feature_map, pad_mask = pad_mask, look_ahead_mask = look_ahead_mask)\n",
    "        out = self.generator(decoder_out) # [batch_size, 16, 54] : 마지막 dimension이 각 자/모음에 대한 확률값\n",
    "        out = F.log_softmax(out, dim = -1)\n",
    "        \n",
    "        return out, decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HangulViT(encoder = CNNEncoder(), decoder = Decoder(embedding_dim=512, num_heads=1, num_layers=6), \n",
    "                  emb_size = 512, nhead = 1, vocab_size = 54, \n",
    "                  num_decoder = 6, dim_feedforward = 2048).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실패\n",
      "'HangulViT' object cannot be interpreted as an integer\n"
     ]
    }
   ],
   "source": [
    "random_image = torch.randn(2, 1, 360, 360).to('cuda')\n",
    "random_src = torch.randint(0, 54, (2, 16)).to('cuda')\n",
    "\n",
    "try:\n",
    "  output = model(random_image, random_src)\n",
    "  print(\"성공\")\n",
    "  print(\"출력 크기: \", output.size())\n",
    "except Exception as e:\n",
    "  print(\"실패\")\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_batch(image_batch, rows=4, cols=8):\n",
    "  fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "  for i, ax in enumerate(axes.flat):\n",
    "    if i < len(image_batch):\n",
    "      ax.imshow(np.asarray(image_batch[i].squeeze(0)))\n",
    "      # ax.set_title(image_labels[i])\n",
    "      ax.axis(\"off\")\n",
    "    else:\n",
    "      ax.axis('off')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 360, 360])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n",
      "tensor([[51, 12, 33, 29, 37, 17, 53, 53, 53, 53],\n",
      "        [51,  6, 36,  2, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 12,  1,  2, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  4, 40, 29, 20, 40, 53, 53, 53, 53],\n",
      "        [51, 17, 47, 18, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  9, 30,  3, 39, 21, 34, 53, 53, 53],\n",
      "        [51, 10, 34, 16, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  8, 38, 10, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  3, 38, 12, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  6, 40,  3, 30, 17, 39,  3, 53, 53],\n",
      "        [51,  8, 40,  3, 49, 12, 37, 17, 53, 53],\n",
      "        [51,  3, 40, 10, 39,  3, 17, 41, 53, 53],\n",
      "        [51, 48, 44, 15, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 11, 31, 10, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  9,  1, 21, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 12, 43, 20, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 15, 40,  5, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 11, 33, 26, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 29, 39, 23, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 13, 31, 53, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 13, 47, 20, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  6, 34, 27, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 50, 36, 53, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  4, 37, 26, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  9, 35, 21, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 20, 40,  9, 39,  3,  0, 40, 20, 40],\n",
      "        [51, 29, 30, 20, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  4, 46, 53, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 13, 30, 29, 37, 17,  0, 40, 20, 40],\n",
      "        [51, 11, 41, 11, 53, 53, 53, 53, 53, 53],\n",
      "        [51, 29, 32,  3, 53, 53, 53, 53, 53, 53],\n",
      "        [51,  8, 43, 20, 53, 53, 53, 53, 53, 53]])\n",
      "tensor([[12, 33, 29, 37, 17, 52, 53, 53, 53, 53],\n",
      "        [ 6, 36,  2, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [12,  1,  2, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 4, 40, 29, 20, 40, 52, 53, 53, 53, 53],\n",
      "        [17, 47, 18, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 9, 30,  3, 39, 21, 34, 52, 53, 53, 53],\n",
      "        [10, 34, 16, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 8, 38, 10, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 3, 38, 12, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 6, 40,  3, 30, 17, 39,  3, 52, 53, 53],\n",
      "        [ 8, 40,  3, 49, 12, 37, 17, 52, 53, 53],\n",
      "        [ 3, 40, 10, 39,  3, 17, 41, 52, 53, 53],\n",
      "        [48, 44, 15, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [11, 31, 10, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 9,  1, 21, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [12, 43, 20, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [15, 40,  5, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [11, 33, 26, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [29, 39, 23, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [13, 31, 52, 53, 53, 53, 53, 53, 53, 53],\n",
      "        [13, 47, 20, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 6, 34, 27, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [50, 36, 52, 53, 53, 53, 53, 53, 53, 53],\n",
      "        [ 4, 37, 26, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 9, 35, 21, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [20, 40,  9, 39,  3,  0, 40, 20, 40, 52],\n",
      "        [29, 30, 20, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 4, 46, 52, 53, 53, 53, 53, 53, 53, 53],\n",
      "        [13, 30, 29, 37, 17,  0, 40, 20, 40, 52],\n",
      "        [11, 41, 11, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [29, 32,  3, 52, 53, 53, 53, 53, 53, 53],\n",
      "        [ 8, 43, 20, 52, 53, 53, 53, 53, 53, 53]])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((360, 360)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),  # 텐서로 변환\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 정규화\n",
    "])\n",
    "\n",
    "# CSV 파일 경로 및 이미지 폴더 설정\n",
    "img_dir = 'D:/dataset/13.한국어글자체/01.손글씨/image'\n",
    "label_file = 'D:/dataset/13.한국어글자체/01.손글씨/new_labels.csv'\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_labels, test_labels = split_dataset(label_file, test_size=0.2, shuffle=True)\n",
    "\n",
    "# train과 test 데이터셋 초기화\n",
    "train_dataset = HangulOCRDataset(img_dir=img_dir, labels=train_labels, transform=transform)\n",
    "test_dataset = HangulOCRDataset(img_dir=img_dir, labels=test_labels, transform=transform)\n",
    "\n",
    "# DataLoader로 셔플 및 배치 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda batch: collate_fn(batch, pad_idx = 53))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=lambda batch: collate_fn(batch, pad_idx = 53))\n",
    "\n",
    "# 예시: 첫 번째 배치 확인\n",
    "images, src_labels, tgt_labels = next(iter(train_loader))\n",
    "# display_image_batch(images)\n",
    "\n",
    "# images, _ = next(iter(test_loader))\n",
    "# display_image_batch(images)\n",
    "\n",
    "print(images.shape)\n",
    "# print(labels[0]) # label은 string 타입\n",
    "# print(src_labels)\n",
    "print(tgt_labels.shape)\n",
    "print(src_labels.shape)\n",
    "print(src_labels)\n",
    "print(tgt_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, epoch, checkpoint_dir):\n",
    "  model.train()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for i, (images, src_labels, tgt_labels) in enumerate(data_loader):\n",
    "    images, src_labels, tgt_labels = images.to(device), src_labels.to(device), tgt_labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(images, src_labels, tgt_labels)\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output.contiguous().view(-1, output_dim) # model의 예측 output\n",
    "    tgt_labels = tgt_labels.contiguous().view(-1) # 실제 정답\n",
    "\n",
    "    loss = criterion(output, tgt_labels)\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  num_samples = idx + 1\n",
    "\n",
    "  if checkpoint_dir:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, f'epoch_{epoch}.pt')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': epoch_loss / num_samples\n",
    "    }, checkpoint_file)\n",
    "\n",
    "  return epoch_loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmod\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mod' is not defined"
     ]
    }
   ],
   "source": [
    "model = Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
